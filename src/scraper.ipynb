{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCRAPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_league_html(session, league, season, window):\n",
    "    \"\"\"\n",
    "    HTML page response\n",
    "    \"\"\"\n",
    "    URL_LEAGUE = \"https://www.transfermarkt.com/premier-league/transfers/wettbewerb/{league}/plus/?saison_id={season}&s_w={window}\"\n",
    "    \n",
    "    scrape_url = URL_LEAGUE.format(\n",
    "        league=league,\n",
    "        season=season,\n",
    "        window=window\n",
    "    )\n",
    "    resp = session.get(scrape_url)\n",
    "    \n",
    "    return BeautifulSoup(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_team_id_from_url(url):\n",
    "    \"\"\"\n",
    "    Helper to prase team id from URL\n",
    "    \"\"\"\n",
    "    if 'verein/' in url:\n",
    "        return url.split('/')[4]\n",
    "    \n",
    "    return None\n",
    "\n",
    "def scrape_transfer_table(transfer_table_soup):\n",
    "    \"\"\"\n",
    "    Scrape data from team transfers HTML table \n",
    "    \"\"\"\n",
    "    table_records = transfer_table_soup.find(\"tbody\").findAll(\"tr\")\n",
    "    \n",
    "    # No rows with player transfers info\n",
    "    if len(table_records[0].findAll(\"td\")) <= 1:\n",
    "        return None\n",
    "    \n",
    "    records = []\n",
    "    for rec in table_records:\n",
    "        rec_data = {}\n",
    "        try:\n",
    "            rec_data['player_name'] = rec.find(\"td\").find(\"div\").text.strip()\n",
    "            rec_data['player_id'] = rec.find(\"td\").find(\"div\").find(\"a\")[\"href\"].split(\"spieler/\")[-1]\n",
    "            rec_data['player_age'] = rec.find(\"td\", class_ = \"zentriert alter-transfer-cell\").text.strip()\n",
    "            \n",
    "            # player nationalities\n",
    "            nationalities = rec_data['player_nat'] = rec.find(\n",
    "                \"td\",\n",
    "                class_ = \"zentriert nat-transfer-cell\"\n",
    "            ).findAll(\"img\")\n",
    "            rec_data['player_nat'] = [nat['title'].strip() for nat in nationalities]\n",
    "        \n",
    "            rec_data['player_pos'] = rec.find(\"td\", class_ = \"kurzpos-transfer-cell zentriert\").text.strip()\n",
    "            rec_data['market_val'] = rec.find(\"td\", class_ = \"rechts mw-transfer-cell\").text.strip()\n",
    "            \n",
    "            # counter team data\n",
    "            counter_team = rec.find(\"td\", class_ = \"verein-flagge-transfer-cell\")\n",
    "            rec_data['counter_team_country'] = counter_team.find(\"img\")[\"title\"].strip() if counter_team.find(\"img\") else counter_team.text.strip()\n",
    "            rec_data['counter_team_name'] = counter_team.find(\"a\")[\"title\"].strip() if counter_team.find(\"a\") else counter_team.text.strip()\n",
    "            rec_data['counter_team_id'] = parse_team_id_from_url(\n",
    "                counter_team.find(\"a\")[\"href\"]\n",
    "            ) if counter_team.find(\"a\") else counter_team.text.strip()\n",
    "            \n",
    "            # transfer data\n",
    "            transfer = rec.findAll(\"td\", class_ = \"rechts\")[-1].find(\"a\")\n",
    "            rec_data['transfer_fee'] = transfer.text.strip()\n",
    "            rec_data['transfer_id'] = transfer[\"href\"].split(\"transfer_id/\")[-1]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(rec)\n",
    "            raise e\n",
    "        \n",
    "        records.append(rec_data)\n",
    "        \n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_league_season(session, league, season, window):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    league_soup = get_league_html(session, league, season, window)\n",
    "    league_country = league_soup.find(\"div\", class_=\"flagge\").find(\"img\")['title'].strip()\n",
    "    \n",
    "    boxes = league_soup.find_all(class_ = \"box\")\n",
    "    team_boxes = []\n",
    "    for box in boxes:\n",
    "        if box.find(\"div\", class_ = \"transfer-zusatzinfo-box\"): # transfer-zusatzinfo-box\n",
    "            team_boxes.append(box)\n",
    "            \n",
    "    data = []\n",
    "    for box in team_boxes:\n",
    "        # team name and ID\n",
    "        team_info_tag = box.find(\"div\", class_ = \"table-header\")\n",
    "        team_name = team_info_tag.text.strip()\n",
    "        team_id = team_info_tag['id'].split('-')[1]\n",
    "\n",
    "        # Tables with transfers\n",
    "        team_tables = box.find_all(\"table\")\n",
    "\n",
    "        # IN transfers\n",
    "        in_transfers = scrape_transfer_table(team_tables[0])\n",
    "\n",
    "        # OUT transfers\n",
    "        out_transfers = scrape_transfer_table(team_tables[1])\n",
    "\n",
    "        data.append(\n",
    "            {\n",
    "                'team': {\n",
    "                    'team_name': team_name,\n",
    "                    'team_id': team_id,\n",
    "                    'team_country': league_country\n",
    "                },\n",
    "                'in': in_transfers,\n",
    "                'left': out_transfers\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_script():\n",
    "    HEADERS = {\n",
    "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "        'accept-encoding': 'gzip, deflate, br',\n",
    "        'accept-language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "        'cache-control': 'max-age=0',\n",
    "        'upgrade-insecure-requests': '1',\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36 OPR/62.0.3331.116',\n",
    "    }\n",
    "    \n",
    "    SCRAPE_LEAGUES = ['GB1', 'ES1', 'IT1', 'L1', 'FR1', 'PO1', 'NL1']\n",
    "    SCRAPE_SEASONS = [2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021]\n",
    "    SCRAPE_WINDOWS = [\"s\", \"w\"]\n",
    "    \n",
    "    session = requests.session()\n",
    "    session.headers.update(HEADERS)\n",
    "    \n",
    "    for league in SCRAPE_LEAGUES:\n",
    "        for season in SCRAPE_SEASONS:\n",
    "            for window in SCRAPE_WINDOWS:\n",
    "            \n",
    "                time.sleep(15)\n",
    "                print('SCRAPE:', league, season, window)\n",
    "\n",
    "                data = scrape_league_season(session, league, season, window)\n",
    "\n",
    "                filename = '../data/{league}_{season}_{window}.json'.format(\n",
    "                    league=league,\n",
    "                    season=season,\n",
    "                    window=window\n",
    "                )\n",
    "                \n",
    "                with open(filename, 'w', encoding='utf8') as output:\n",
    "                    json.dump(data, output, ensure_ascii=False)\n",
    "                \n",
    "                \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN SCRAPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCRAPE: ES1 2009 s\n",
      "SCRAPE: ES1 2009 w\n",
      "SCRAPE: ES1 2010 s\n",
      "SCRAPE: ES1 2010 w\n",
      "SCRAPE: ES1 2011 s\n",
      "SCRAPE: ES1 2011 w\n",
      "SCRAPE: ES1 2012 s\n",
      "SCRAPE: ES1 2012 w\n",
      "SCRAPE: ES1 2013 s\n",
      "SCRAPE: ES1 2013 w\n",
      "SCRAPE: ES1 2014 s\n",
      "SCRAPE: ES1 2014 w\n",
      "SCRAPE: ES1 2015 s\n",
      "SCRAPE: ES1 2015 w\n",
      "SCRAPE: ES1 2016 s\n",
      "SCRAPE: ES1 2016 w\n",
      "SCRAPE: ES1 2017 s\n",
      "SCRAPE: ES1 2017 w\n",
      "SCRAPE: ES1 2018 s\n",
      "SCRAPE: ES1 2018 w\n",
      "SCRAPE: ES1 2019 s\n",
      "SCRAPE: ES1 2019 w\n",
      "SCRAPE: ES1 2020 s\n",
      "SCRAPE: ES1 2020 w\n",
      "SCRAPE: ES1 2021 s\n",
      "SCRAPE: ES1 2021 w\n",
      "SCRAPE: IT1 2009 s\n",
      "SCRAPE: IT1 2009 w\n",
      "SCRAPE: IT1 2010 s\n",
      "SCRAPE: IT1 2010 w\n",
      "SCRAPE: IT1 2011 s\n",
      "SCRAPE: IT1 2011 w\n",
      "SCRAPE: IT1 2012 s\n",
      "SCRAPE: IT1 2012 w\n",
      "SCRAPE: IT1 2013 s\n",
      "SCRAPE: IT1 2013 w\n",
      "SCRAPE: IT1 2014 s\n",
      "SCRAPE: IT1 2014 w\n",
      "SCRAPE: IT1 2015 s\n",
      "SCRAPE: IT1 2015 w\n",
      "SCRAPE: IT1 2016 s\n",
      "SCRAPE: IT1 2016 w\n",
      "SCRAPE: IT1 2017 s\n",
      "SCRAPE: IT1 2017 w\n",
      "SCRAPE: IT1 2018 s\n",
      "SCRAPE: IT1 2018 w\n",
      "SCRAPE: IT1 2019 s\n",
      "SCRAPE: IT1 2019 w\n",
      "SCRAPE: IT1 2020 s\n",
      "SCRAPE: IT1 2020 w\n",
      "SCRAPE: IT1 2021 s\n",
      "SCRAPE: IT1 2021 w\n",
      "SCRAPE: L1 2009 s\n",
      "SCRAPE: L1 2009 w\n",
      "SCRAPE: L1 2010 s\n",
      "SCRAPE: L1 2010 w\n",
      "SCRAPE: L1 2011 s\n",
      "SCRAPE: L1 2011 w\n",
      "SCRAPE: L1 2012 s\n",
      "SCRAPE: L1 2012 w\n",
      "SCRAPE: L1 2013 s\n",
      "SCRAPE: L1 2013 w\n",
      "SCRAPE: L1 2014 s\n",
      "SCRAPE: L1 2014 w\n",
      "SCRAPE: L1 2015 s\n",
      "SCRAPE: L1 2015 w\n",
      "SCRAPE: L1 2016 s\n",
      "SCRAPE: L1 2016 w\n",
      "SCRAPE: L1 2017 s\n",
      "SCRAPE: L1 2017 w\n",
      "SCRAPE: L1 2018 s\n",
      "SCRAPE: L1 2018 w\n",
      "SCRAPE: L1 2019 s\n",
      "SCRAPE: L1 2019 w\n",
      "SCRAPE: L1 2020 s\n",
      "SCRAPE: L1 2020 w\n",
      "SCRAPE: L1 2021 s\n",
      "SCRAPE: L1 2021 w\n",
      "SCRAPE: FR1 2009 s\n",
      "SCRAPE: FR1 2009 w\n",
      "SCRAPE: FR1 2010 s\n",
      "SCRAPE: FR1 2010 w\n",
      "SCRAPE: FR1 2011 s\n",
      "SCRAPE: FR1 2011 w\n",
      "SCRAPE: FR1 2012 s\n",
      "SCRAPE: FR1 2012 w\n",
      "SCRAPE: FR1 2013 s\n",
      "SCRAPE: FR1 2013 w\n",
      "SCRAPE: FR1 2014 s\n",
      "SCRAPE: FR1 2014 w\n",
      "SCRAPE: FR1 2015 s\n",
      "SCRAPE: FR1 2015 w\n",
      "SCRAPE: FR1 2016 s\n",
      "SCRAPE: FR1 2016 w\n",
      "SCRAPE: FR1 2017 s\n",
      "SCRAPE: FR1 2017 w\n",
      "SCRAPE: FR1 2018 s\n",
      "SCRAPE: FR1 2018 w\n",
      "SCRAPE: FR1 2019 s\n",
      "SCRAPE: FR1 2019 w\n",
      "SCRAPE: FR1 2020 s\n",
      "SCRAPE: FR1 2020 w\n",
      "SCRAPE: FR1 2021 s\n",
      "SCRAPE: FR1 2021 w\n",
      "SCRAPE: PO1 2009 s\n",
      "SCRAPE: PO1 2009 w\n",
      "SCRAPE: PO1 2010 s\n",
      "SCRAPE: PO1 2010 w\n",
      "SCRAPE: PO1 2011 s\n",
      "SCRAPE: PO1 2011 w\n",
      "SCRAPE: PO1 2012 s\n",
      "SCRAPE: PO1 2012 w\n",
      "SCRAPE: PO1 2013 s\n",
      "SCRAPE: PO1 2013 w\n",
      "SCRAPE: PO1 2014 s\n",
      "SCRAPE: PO1 2014 w\n",
      "SCRAPE: PO1 2015 s\n",
      "SCRAPE: PO1 2015 w\n",
      "SCRAPE: PO1 2016 s\n",
      "SCRAPE: PO1 2016 w\n",
      "SCRAPE: PO1 2017 s\n",
      "SCRAPE: PO1 2017 w\n",
      "SCRAPE: PO1 2018 s\n",
      "SCRAPE: PO1 2018 w\n",
      "SCRAPE: PO1 2019 s\n",
      "SCRAPE: PO1 2019 w\n",
      "SCRAPE: PO1 2020 s\n",
      "SCRAPE: PO1 2020 w\n",
      "SCRAPE: PO1 2021 s\n",
      "SCRAPE: PO1 2021 w\n",
      "SCRAPE: NL1 2009 s\n",
      "SCRAPE: NL1 2009 w\n",
      "SCRAPE: NL1 2010 s\n",
      "SCRAPE: NL1 2010 w\n",
      "SCRAPE: NL1 2011 s\n",
      "SCRAPE: NL1 2011 w\n",
      "SCRAPE: NL1 2012 s\n",
      "SCRAPE: NL1 2012 w\n",
      "SCRAPE: NL1 2013 s\n",
      "SCRAPE: NL1 2013 w\n",
      "SCRAPE: NL1 2014 s\n",
      "SCRAPE: NL1 2014 w\n",
      "SCRAPE: NL1 2015 s\n",
      "SCRAPE: NL1 2015 w\n",
      "SCRAPE: NL1 2016 s\n",
      "SCRAPE: NL1 2016 w\n",
      "SCRAPE: NL1 2017 s\n",
      "SCRAPE: NL1 2017 w\n",
      "SCRAPE: NL1 2018 s\n",
      "SCRAPE: NL1 2018 w\n",
      "SCRAPE: NL1 2019 s\n",
      "SCRAPE: NL1 2019 w\n",
      "SCRAPE: NL1 2020 s\n",
      "SCRAPE: NL1 2020 w\n",
      "SCRAPE: NL1 2021 s\n",
      "SCRAPE: NL1 2021 w\n"
     ]
    }
   ],
   "source": [
    "scrape_script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
